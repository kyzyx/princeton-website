<html>
    <head>
        <title>
            Kinect SDK C++ Tutorials - 3. Point Clouds
        </title>
        <link rel="stylesheet" type="text/css" href="../main.css">
        <link href="../google-code-prettify/prettify.css" type="text/css" rel="stylesheet" />
        <script type="text/javascript" src="../google-code-prettify/prettify.js"></script>
    </head>
    <body onload="prettyPrint()">
    <div id='navbarcontainer'>
        <ul id='navbar'>
            <li class='navitem'><a class='nav' href='../../index.html'>Home</a></li>
            <li class='navitem'><a class='nav' href='../index.html'>Tutorials Home</a></li>
        </ul>
    </div>
    <div id='content'>
        <h1> Kinect v2 SDK C++ - 3. Kinect Point Clouds </h1>
        <p>
        <b>Goals:</b> Learn how to align color and depth images to get
        a colored point cloud. <br>
        <b>Source:</b> <a href='https://github.com/kyzyx/Tutorials/tree/master/KinectSDK/3_PointCloud'>View Source</a> &nbsp;&nbsp;&nbsp;&nbsp; <b>Download:</b> <a href='3_PointCloud.zip'>3_PointCloud.zip</a>

        <hr>
        <h2>Overview</h2>
        There are several new steps we want to take in this tutorial. The most
        interesting part is that now we're working with 3D data! Creating an
        interactive system is a bit too much code for us, though, so we just
        have a simple rotating point cloud.

        This tutorial has three parts: first, we'll talk briefly about why
        point clouds are harder than you might think.
        Then, we'll show the Kinect SDK side of how to get the right data.
        Finally, we'll show some OpenGL tricks to make things easy to display.
        <br>
        <h3>Contents</h3>
        <ol>
            <li><a href='#coordinates'>Depth and RGB Coordinate Systems</a></li>
            <li><a href='#kinect'>Kinect Code</a></li>
            <li><a href='#opengl'>OpenGL Display</a></li>
            <li><a href='#puttingittogether'>Putting it all together</a></li>
        </ol>
        <hr>
        <h2><a name='coordinates'>Depth and RGB Coordinate Systems</a></h2>
        <h3> Kinect Coordinate System </h3>
        The Kinect uses a cartesian coordinate system centered at the Kinect's IR sensor. The positive Y axis points up,
        the positive Z axis points where the Kinect is pointing, and the positive X axis is to the left. One unit in this
        coordinate system is equal to one meter.
        <br>
        <img src="https://i-msdn.sec.s-msft.com/dynimg/IC757720.png">
        <br>
        <h3>Alignment</h3>
        A naive way of making a point cloud might directly overlap the depth and color images, so that depth pixel (x,y) goes with image pixel (x,y).
        However, this would give you a poor quality depth map, where the borders of objects don't line up with the colors. This occurs because the RGB
        camera and the depth camera are located at different spots on the Kinect; obviously, then, they aren't seeing the same things! Normally, we'd
        have to do some kind of alignment of the two cameras (the formal term is registration) so that we know what color pixel goes with what
        depth pixel. Fortunately Microsoft can already do this for us, so all we need to do is call the right functions.
        <table>
        <tr>
            <td><img width="100%" src="kinect3_unregistered.png"></td><td><img width="100%" src="kinect3_registered.png"></td>
        </tr>
        <td><center>Direct overlap of RGB and Depth</center></td><td><center>Registered RGB and Depth</center></td>
        <tr>
        </tr>
        </table>
        Note that computer vision and robotics researchers don't like the quality of the built-in registration, so they often do it manually using
        something like <a href="http://opencv.org/">OpenCV</a>.
        <hr>
        <h2><a name='kinect'>Kinect Code</a></h2>
        There are two new issues that we have to deal with, and correspondingly two
        new interfaces:

        <ul>
            <li>
            Up until now, we've only had to deal with a single data stream at a time.
            However, now that we're using both color and depth data (and possibly other
            types, such as IR or body-tracking data), we want to make sure that the
            data we're processing is synchronized. Otherwise, the color image could show
            one part of the scene while the depth data shows another!
            <br>
            Instead, we will use a <code>MultiSourceFrameReader</code>, which gives
            <code>MultiSourceFrame</code>s.
            </li>
            <li>
            To deal with the different coordinate systems mentioned above, the SDK
            provides an object that will do all the conversion work for you:
            <code>ICoordinateMapper</code>.
            </li>
        </ul>
        <h3>Kinect Initialization</h3>
        <pre class="prettyprint">
IKinectSensor* sensor;             // Kinect sensor
IMultiSourceFrameReader* reader;   // Kinect data source
ICoordinateMapper* mapper;         // Converts between depth, color, and 3d coordinates

bool initKinect() {
    if (FAILED(GetDefaultKinectSensor(&sensor))) {
        return false;
    }
    if (sensor) {
        sensor->get_CoordinateMapper(&mapper);

        sensor->Open();
        sensor->OpenMultiSourceFrameReader(
            FrameSourceTypes::FrameSourceTypes_Depth | FrameSourceTypes::FrameSourceTypes_Color,
            &reader);
        return reader;
    } else {
        return false;
    }
}
        </pre>
        We open the <code>MultiSourceFrameReader</code> by specifying which
        data we want in our frames. We can also request IR data, body tracking
        data, and audio data. We don't need to deal with FrameSources separately from FrameReaders.
        <p>
        We also initialize our <code>ICoordinateMapper</code> here.
        <h3>Getting Data from a MultiSourceFrame</h3>
        For both the depth data and the color data, we can get the familiar
        <code>IDepthFrame</code> and <code>IColorFrame</code> from the <code>MultiSourceFrame</code>.
        Here is the code for the depth data:
        <pre class="prettyprint">
void getKinectData() {
    IMultiSourceFrame* frame = NULL;
    reader->AcquireLatestFrame(&frame);
    // ...
    getDepthData(frame, dest);
    // ...
    getColorData(frame, dest;
}

void getDepthData(IMultiSourceFrame* frame, GLubyte* dest) {
    IDepthFrame* depthframe;
    IDepthFrameReference* frameref = NULL;
    frame->get_DepthFrameReference(&frameref);
    frameref->AcquireFrame(&depthframe);
    if (frameref) frameref->Release();
    if (!depthframe) return;

    // Process depth frame data...

    if (depthframe) depthframe->Release();
}
        </pre>

        The code is almost identical for color data; just replace all
        occurrences of "depth" with "color"

        <h3>Using the Coordinate Mapper Interface</h3>
        We are dealing with three different coordinate spaces in this program.
        <ul>
            <li>The 3D (XYZ) space, where the point cloud coordinates are in, is used for display.</li>
            <li>The 2D (column, row) space of pixel coordinates in the 1920*1080 color image.</li>
            <li>The 2D (column, row) space of pixel coordinates in the 512*424 depth image.</li>
        </ul>
         We will display one 3D point for every pixel in the depth image. Each
         of these points will have an RGB color and then an XYZ coordinates.
         Therefore, we will use the <code>CoordinateMapper</code> to get a lookup
         table mapping between depth pixel coordinates and 3D point coordinates,
         and another one mapping between depth pixel coordinates to the corresponding
         pixel in the color image.

        <pre class="prettyprint">
// Global Variables
/*
 For reference:
 struct ColorSpacePoint { float X, Y; };
 struct CameraSpacePoint { float X, Y, Z; };
*/
ColorSpacePoint depth2rgb[width*height];     // Maps depth pixels to rgb pixels
CameraSpacePoint depth2xyz[width*height];    // Maps depth pixels to 3d coordinates

// ...

void getDepthData(IMultiSourceFrame* frame, GLubyte* dest) {
    IDepthFrame* depthframe;
    // Populate depthframe from MultiSourceFrame...

    // Get data from frame
    unsigned int sz;
    unsigned short* buf;
    depthframe->AccessUnderlyingBuffer(&sz, &buf);

    mapper->MapDepthFrameToCameraSpace(
            width*height, buf,        // Depth frame data and size of depth frame
            width*height, depth2xyz); // Output CameraSpacePoint array and size
    mapper->MapDepthFrameToColorSpace(
            width*height, buf,        // Depth frame data and size of depth frame
            width*height, depth2rgb); // Output ColorSpacePoint array and size
}
        </pre>
        To get the mappings, we call into the appropriate <code>ICoordinateMapper</code>
        functions. You can find other possible mappings in the
        <a href='https://msdn.microsoft.com/en-us/library/windowspreview.kinect.coordinatemapper.aspx'><code>ICoordinateMapper</code> documentation</a>
        Note that most mapping functions require the depth frame as an input array
        (even the ones that don't start with "DepthFrame" in the name).

        <h3> Getting depth data from the Kinect </h3>
        Now that we're dealing with 3D data, we want to imagine the depth frame
        as a bunch of points in space rather than a 512x424 image. So in our
        <code>getDepthData</code> function, we will fill in our buffer with
        the coordinates of each point (instead of the depth at each pixel).
        This means the buffer we pass into it has to have size
        <code>width*height*3*sizeof(float)</code> for <code>float</code>
        typed coordinates. Here, we simply use the <code>depth2xyz</code>
        map that we got from the <code>CoordinateMapper</code>.
        <pre class="prettyprint">
void getDepthData(IMultiSourceFrame* frame, GLubyte* dest) {
// Populate depth2xyz map...
    float* fdest = (float*)dest;
    for (int i = 0; i < width*height i++) {
        *fdest++ = depth2xyz[i].X;
        *fdest++ = depth2xyz[i].Y;
        *fdest++ = depth2xyz[i].Z;
    }
/* We use the fdest pointer for conciseness. Equivalently, we could use
    for (int i = 0; i < width*height; i++) {
        fdest[3*i+0] = depth2xyz[i].X;
        fdest[3*i+1] = depth2xyz[i].Y;
        fdest[3*i+2] = depth2xyz[i].Z;
    }
*/
}
</pre>

        <h3> Getting color data from the Kinect </h3>
        Now that we are thinking about things in terms of points instead of
        rectangular grids, we want our color output to be associated with a
        particular depth point. In particular, the input to our
        <code>getRgbData</code> function, analogously to the
        <code>getDepthData</code> function, wants a buffer of size
        <code>width*height*3*sizeof(float)</code> to hold the
        red, green, and blue values for each point in our point cloud.
        <pre class="prettyprint">
void getRgbData(IMultiSourceFrame* frame, GLubyte* dest) {
    IColorFrame* colorframe;

    // Populate colorframe...

    colorframe->CopyConvertedFrameDataToArray(colorwidth*colorheight*4, rgbimage, ColorImageFormat_Rgba);

    // Write color array for vertices
    float* fdest = (float*)dest;
    for (int i = 0; i < width*height; i++) {
        ColorSpacePoint p = depth2rgb[i];
        // Check if color pixel coordinates are in bounds
        if (p.X < 0 || p.Y < 0 || p.X > colorwidth || p.Y > colorheight) {
            *fdest++ = 0;
            *fdest++ = 0;
            *fdest++ = 0;
        }
        else {
            int idx = (int)p.X + colorwidth*(int)p.Y;
            *fdest++ = rgbimage[4*idx + 0]/255.;
            *fdest++ = rgbimage[4*idx + 1]/255.;
            *fdest++ = rgbimage[4*idx + 2]/255.;
        }
        // Don't copy alpha channel
    }
    /* We use fdest pointer for conciseness; Equivalently, we could use

    for (int i = 0; i < width*height; i++) {
        fdest[3*i+0] = ...
        fdest[3*i+1] = ...
        fdest[3*i+2] = ...
    }
     */
 }
        </pre>
        In this block, we iterate through the pixels of the depth image,
        looking up the associated coordinates in the color image using
        the <code>depth2rgb</code> map we obtained from the <code>CoordinateMapper</code>.
        We check whether the depth pixel actually projects to a pixel in the RGB
        image, and if it does not, then we just assign that point a black color.
        <br>
        There's a bit of funny math in those last couple of lines, so let's walk
        through it. First of all, the color image frame is in RGBA format, one
        byte per channel, laid out row by row. So the linear index for pixel
        (x,y) is <code>x + width*y</code>. X and Y can be floating point, so we
        round them down before using them as an array index by casting to int.
        Then, the 4-byte block we want is at <code>linearindex*4</code>.
        Finally, we want to convert from byte-valued (0-255) RGBA into
        float-valued (0.0-1.0) RGB, so we take the first 3 channels and divide
        by 255: <code>rgbimage[4*linearindex + channel]/255.f</code>.

        <hr>
        <h2><a name='opengl'>OpenGL Display</a></h2>
        We're going to use array buffers to display our point cloud. What are
        array buffers? They let you replace a series of <code>glBegin, glColor, glVertex, glEnd</code> calls with a single function call. As a bonus, the
        array buffers are stored on the GPU so displaying them is more efficient. They do make the code a bit more complicated, though.
        Want to skip the array buffers? Go <a href="#noarraybuffers">here</a>.
        <p>
        To use array buffers, we have to deal with OpenGL extensions. To make this
        easier, we use GLEW.
        <h3> Installing GLEW </h3>
        <ol>
            <li>Download and unzip the GLEW binaries from <a href='http://glew.sourceforge.net/'>http://glew.sourceforge.net/</a></li>
            <li>Copy the contents of the Include/ and Lib/ directories you just
            unzipped into the appropriate Windows SDK directories. e.g.
                <ul>
                    <li> C:/Program Files/Microsoft SDKs/Windows/v7.0A/Include/ and
                    C:/Program Files/Microsoft SDKs/Windows/v7.0A/Lib/ for Visual Studio 2010 </li>
                    <li>C:/Program Files/Windows Kits (x86)/8.1/Include/um/ and
                    C:/Program Files (x86)/Windows Kits/8.1/Lib/winv6.3/um/ for Visual Studio 2012+</li>
                </ul>
            </li>
            <li>Copy bin/x64/glew32.dll into C:/Windows/System32 and bin/x86/glew32.dll into C:/Windows/SysWOW64. If
            you have a 32-bit system, just move bin/x86/glew32.dll into C:/Windows/System32</li>
        </ol>
        Add glew32.lib to the SDL or OpenGL property sheet's Linker > Input > Additional Dependencies.

        <h3>OpenGL Code</h3>
        Since we're dealing with 3D data, we now also have to worry about
        camera settings. We use a <code>gluPerspective</code> and <code>gluLookAt</code> to deal with that for us.
        <pre class="prettyprint">
// Global variables:
GLuint vboId; // Vertex buffer ID
GLuint cboId; // Color buffer ID

// ...
    // OpenGL setup
    glClearColor(0,0,0,0);
    glClearDepth(1.0f);

    // Set up array buffers
    const int dataSize = width*height * 3 * 4;
    glGenBuffers(1, &vboId);
    glBindBuffer(GL_ARRAY_BUFFER, vboId);
    glBufferData(GL_ARRAY_BUFFER, dataSize, 0, GL_DYNAMIC_DRAW);
    glGenBuffers(1, &cboId);
    glBindBuffer(GL_ARRAY_BUFFER, cboId);
    glBufferData(GL_ARRAY_BUFFER, dataSize, 0, GL_DYNAMIC_DRAW);

    // Camera setup
    glViewport(0, 0, width, height);
    glMatrixMode(GL_PROJECTION);
    glLoadIdentity();
    gluPerspective(45, width /(GLdouble) height, 0.1, 1000);
    glMatrixMode(GL_MODELVIEW);
    glLoadIdentity();
    gluLookAt(0,0,0,0,0,1,0,1,0);
        </pre>

        For display purposes, rather than having a fully interactive setup we
        just have a rotating camera that rotates around the point 3 meters in
        front of the Kinect. See the code for details.

        <hr>
        <h2><a name='puttingittogether'>Putting it all together</a></h2>
        We wrote those nice functions getDepthData and getRgbData, but how
        do we use them? We want to copy the appropriate data from our
        MultiSourceFrame onto the GPU.
        <pre class="prettyprint">
void getKinectData() {
    IMultiSourceFrame* frame = NULL;
    if (SUCCEEDED(reader->AcquireLatestFrame(&frame))) {
        GLubyte* ptr;
        glBindBuffer(GL_ARRAY_BUFFER, vboId);
        ptr = (GLubyte*)glMapBuffer(GL_ARRAY_BUFFER, GL_WRITE_ONLY);
        if (ptr) {
            getDepthData(frame, ptr);
        }
        glUnmapBuffer(GL_ARRAY_BUFFER);
        glBindBuffer(GL_ARRAY_BUFFER, cboId);
        ptr = (GLubyte*)glMapBuffer(GL_ARRAY_BUFFER, GL_WRITE_ONLY);
        if (ptr) {
            getRgbData(frame, ptr);
        }
        glUnmapBuffer(GL_ARRAY_BUFFER);
    }
    if (frame) frame->Release();
}
        </pre>

        Now we want to use the <code>glDrawArrays</code> function to draw our
        point cloud.
        <pre class="prettyprint">
void drawKinectData() {
    getKinectData();
    rotateCamera();

    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    glEnableClientState(GL_VERTEX_ARRAY);
    glEnableClientState(GL_COLOR_ARRAY);

    glBindBuffer(GL_ARRAY_BUFFER, vboId);
    glVertexPointer(3, GL_FLOAT, 0, NULL);

    glBindBuffer(GL_ARRAY_BUFFER, cboId);
    glColorPointer(3, GL_FLOAT, 0, NULL);

    glPointSize(1.f);
    glDrawArrays(GL_POINTS, 0, width*height);

    glDisableClientState(GL_VERTEX_ARRAY);
    glDisableClientState(GL_COLOR_ARRAY);
}
    </pre>
    <a name='noarraybuffers'>
    Note that we could just as well replace all the array buffer code with</a>
    <pre class="prettyprint">
// Global Variables
float colorarray[width*height*3];
float vertexarray[width*height*3];
//...
void getKinectData() {
    getDepthData((*GLubyte*) vertexarray);
    getRgbData((GLubyte*) colorarray);
}
void drawKinectData() {
    getKinectData();
    rotateCamera();
    glBegin(GL_POINTS);
    for (int i = 0; i < width*height; ++i) {
        glColor3f(colorarray[i*3], colorarray[i*3+1], colorarray[i*3+2]);
        glVertex3f(vertexarray[i*3], vertexarray[i*3+1], vertexarray[i*3+2]);
    }
    glEnd();
}
    </pre>

        <hr>
        The End! Build and run, making sure that your Kinect is plugged in.
        You should see a window containing a rotating color point cloud of
        what your kinect sees.
        <br>
        <img src='kinect3.gif'>
        <hr>
<table width="100%"><tr><td>
        <a href='kinect2.html'>Previous: Depth Images</a>
</td><td>
        <p align="right">
        <a href='#'>Next: Skeletal Data (Coming Soon)</a>
        </p>
</td>
    </div>
    </body>
</html>
